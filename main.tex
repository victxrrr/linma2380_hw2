\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[colorlinks, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
\usepackage{geometry}
\geometry{tmargin=2cm, bmargin=2cm, lmargin=2cm, rmargin=2cm}
\usepackage{todonotes} %Used for the figure placeholders
\usepackage{ifthen}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{verbatim, fancyvrb}
\usepackage{cprotect}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{changepage}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepackage{url} 
\usepackage{enumitem}

\usepackage[ backend=biber, style=alphabetic, sorting=ynt ]{biblatex}

\addbibresource{mybibliography.bib}

\DeclareMathOperator*{\argmin}{arg\,min}
%\counterwithin{equation}{subsection}
%\renewcommand{\theequation}{\thesubsection\alph{equation}} \makeatletter
%\renewcommand{\p@equation}{\thesection.\thesubsection} \makeatother

% Your name and student number must be filled in on the title page found in
% titlepage.tex.


\newcommand{\supsuboverbrace}[2][]{%
  {\everymath{\scriptstyle}%
   \overbrace{\scriptstyle#2}^{#1}}%
}
%\numberwithin{equation}{section}

\begin{document}
\setlist[enumerate]{leftmargin=0cm}

\VerbatimFootnotes
\newboolean{anonymize}
% Uncomment to create an anonymized version of your report
%\setboolean{anonymize}{true}

\begin{titlepage}
    \newpage
    \thispagestyle{empty}
    \frenchspacing
    \hspace{-0.2cm}
    \includegraphics[height=4cm]{sedes}
    \hspace{0.2cm}
    \rule{0.5pt}{4cm}
    \hspace{0.2cm}
    \begin{minipage}[b]{8cm}
        \Large{Université\newline Catholique\newline de
        Louvain}\smallskip\newline
        \large{}\smallskip \smallskip \newline
        \textbf{Ecole \newline Polytechnique \newline de  Louvain}\smallskip
        \smallskip
    \end{minipage}
    \hspace{\stretch{1}} \vspace*{3.2cm}\vfill
    \begin{center}
        \begin{minipage}[t]{\textwidth}
            \begin{center}
                \LARGE{\rm{\textbf{\uppercase{Matrix Computations}}}}\\
                \Large{\rm{Homework 2}}
            \end{center}
        \end{minipage}
    \end{center}
    \vfill
    \hfill\makebox[8.5cm][l]{%
        \vbox to 7cm{\vfill\noindent \ifthenelse{\boolean{anonymize}}{%
                {\rm \textbf{Anonymized}}\\
                {\rm Academic year 2016--2017} }{%
                {\rm \textbf{Jonathan Kojovi Victor}}\\
                {\rm 58522100}\\
                {\rm \textbf{Antoine Lemaire}} \\
                {\rm 16732000} \\
                {\rm \textbf{Margot Devillers}} \\
                {\rm 28542000} \\
                {\rm \textbf{Victor Lepère}}\\
                {\rm 61502000}\\[2mm]
                {\rm Academic year 2024--2025} } } }
\end{titlepage}

\newpage



\section*{Exercise A: Krylov subspaces}

\subsection*{A1}

For all $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n$ and $r \in \mathbb{N}_{>0}$, we know that the \textit{Krylov subspace} of order $r$ generated by $A$ and $b$ is defined by 
\begin{align*}
    \mathcal{K}_r(A,b) = \text{span}\{b,Ab,\dots,A^{r-1}b\} \; .
\end{align*}

Let us now take any $s \in \mathbb{N}_{>0}$ such that $s \geq r$. Since $s \geq r$, we have:
\begin{align*}
    \mathcal{K}_s(A,b) = \text{span}\{b,Ab,\dots,A^{r-1}b,A^rb,A^{r+1}b,\dots,A^{s-1}b\} \; ,
\end{align*}
and so all the vectors in the span of $\mathcal{K}_r(A,b)$ must also be in the span of $\mathcal{K}_s(A,b)$. In addition, since $\mathcal{K}_s(A,b)$ has other vectors which are not in the span of $\mathcal{K}_r(A,b)$, we must have that $\mathcal{K}_r(A,b) \subseteq \mathcal{K}_s(A,b)$. Since this is true for any $s \in \mathbb{N}_{>0}$ with $s \geq r$, it is also true if we take $s = r+1$, so $\mathcal{K}_r(A,b) \subseteq \mathcal{K}_{r+1}(A,b)$. \\

With this, and under the assumption that for any $r \in \mathbb{N}_{>0}$, $\mathcal{K}_{r+1}(A,b) \subseteq \mathcal{K}_r(A,b)$, we must have that $\mathcal{K}_r(A,b) = \mathcal{K}_{r+1}(A,b)$. In a similar way, we also have that $\mathcal{K}_{r+1}(A,b) = \mathcal{K}_{r+2}(A,b)$, which means that $\mathcal{K}_r(A,b) = \mathcal{K}_{r+2}(A,b)$. By induction this means that $\mathcal{K}_r(A,b) = \mathcal{K}_s(A,b)$ for all $s \geq r$. \\
\begin{flushright}
    $\qed$
\end{flushright}


\subsection*{A2}

Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$ and $s = \text{dim}(\mathcal{K}_n(A,b))$. To prove that for all $1 \leq r \leq s$, $\text{dim}(\mathcal{K}_r(A,b)) = r$, we will proceed by induction. \\

First, as base case, we will use $r=1$. In this case, we have:
\begin{align*}
    \mathcal{K}_1(A,b) = \text{span}\{b\} \ .
\end{align*}
Since $b \neq 0$, we indeed have that $\text{dim}(\mathcal{K}_1(A,b)) = 1$, and so the base case holds. \\

Next, we will use the induction step and prove that if our statement holds for some $1 \leq r < s$, then it also holds for $r+1$. So, we will prove that if we assume that for some $1 \leq r < s$, $\text{dim}(\mathcal{K}_r(A,b)) = r$, then we will also have $\text{dim}(\mathcal{K}_{r+1}(A,b)) = r+1$. \\

Since $\mathcal{K}_r(A,b) = \text{span}\{b,Ab,\dots,A^{r-1}b\}$, if we assume that $\text{dim}(\mathcal{K}_r(A,b)) = r$, that means that the vectors $b,Ab,\dots,A^{r-1}b$ are linearly independent. In order to show that $\text{dim}(\mathcal{K}_{r+1}(A,b)) = r+1$, we need to show that $A^rb$ is linearly independent of $\{b,Ab,\dots,A^{r-1}\}$. Let's do this by contradiction. Suppose $A^rb$ can be expressed as a linear combination of the vectors of $\{b,Ab,\dots,A^{r-1}\}$, so:
\begin{align*}
    A^rb = c_0 b + c_1 Ab + \dots + c_{r-1}A^{r-1}b
\end{align*}
for some $c_0,c_1,\dots,c_{r-1} \in \mathbb{R}$. This would mean that $A^rb \in \mathcal{K}_r(A,b)$, and so $\mathcal{K}_{r+1}(A,b) = \mathcal{K}_r(A,b)$. From this, we can of course also write $\mathcal{K}_{r+1}(A,b) \subseteq \mathcal{K}_r(A,b)$. And so from the proof of section \textbf{A1}, we know that for all $s \geq r$, it holds that $\mathcal{K}_s(A,b) = \mathcal{K}_r(A,b)$. In addition, since in our induction step we used $r < s$ and since $s = \text{dim}(\mathcal{K}_n(A,b)) \leq n$, we have that $n \geq r$ and so $\mathcal{K}_n(A,b) = \mathcal{K}_r(A,b)$. Because of this, we have that $\text{dim}(\mathcal{K}_n(A,b)) = \text{dim}(\mathcal{K}_r(A,b)) = r$ from our initial assumption, and so this leads to a contradiction since by definition, $\text{dim}(\mathcal{K}_n(A,b)) = s > r$. Since we have a contradiction, we must have that $A^rb$ is not linearly dependent of $\{b,Ab,\dots,A^{r-1}b\}$, and so $\text{dim}(\mathcal{K}_{r+1}(A,b)) = r+1$. \\

This concludes our induction step, and with our base case used earlier, this concludes the proof that if $s = \text{dim}(\mathcal{K}_n(A,b))$, then for all $1 \leq r \leq s$, it holds that $\text{dim}(\mathcal{K}_r(A,b)) = r$.
\begin{flushright}
    $\qed$
\end{flushright}



\section*{Exercise B: Arnoldi’s iteration}

\subsection*{B1}

Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$ and let $(Q_s,R_s)$ be a QR decomposition of $K_s(A,b)$, where $K_s(A,b) = [b,Ab,\dots,A^{s-1}b]$. $R_s \in \mathbb{R}^{s \times s}$ is an upper-triangular matrix whose diagonal entries represent the norms of the orthogonalized columns of $K_s(A,b)$ (by definition of the QR decomposition). More specifically, the first diagonal entry of $R_s$ is the norm of the first column of $K_s(A,b)$. The second diagonal entry of $R_s$ is the norm of the second column of $K_s(A,b)$ after removing the component that is parallel to the column. And so on for the following diagonal entries. \\

By letting $s = \text{dim}(\mathcal{K}_n(A,b))$, we know from section \textbf{A2} that for all $1 \leq r \leq s$, it holds that $\text{dim}(\mathcal{K}_r(A,b)) = r$. This is thus true for $r = s$, in which case we have $\text{dim}(\mathcal{K}_s(A,b)) = s$. This means that the matrix $K_s(A,b)$ has full column rank and so all of its columns are linearly independent. Because of this, and from what we have said in the previous paragraph, we know that $R_s$ must have nonzero elements on its diagonal. Indeed, as we have said previously, the first diagonal entry of $R_s$ is the norm of the first column of $K_s(A,b)$ which is simply the norm of $b$ which is nonzero since $b \in \mathbb{R}^n \setminus \{0\}$. The second diagonal entry of $R_s$ is the norm of the second column of $K_s(A,b)$ after it has been orthonalized with respect to the first column of $K_s(A,b)$. So, this element must be nonzero since the two first columns of $K_s(A,b)$ are linearly independent. Since all the columns of $K_s(A,b)$ are linearly independent as we have proven above, this indeed means that none of the diagonal entries of $R_s$ will be zero. 
\begin{flushright}
    $\qed$
\end{flushright}



\subsection*{B2}

Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, $s = \text{dim}(\mathcal{K}_n(A,b))$ and $(Q_s,R_s)$ be a QR decomposition of $K_s(A,b)$. From section \textbf{A2} we know that for all $1 \leq r \leq s$, it holds that $\text{dim}(\mathcal{K}_r(A,b)) = r$. Thanks to this, we know that $\mathcal{K}_r(A,b)$ and $\mathcal{K}_s(A,b)$ both have full column rank. Additionally, since
\begin{align*}
    \mathcal{K}_r(A,b) &= \text{span}\{ b,Ab,\dots,A^{r-1}b \} \\
    \mathcal{K}_s(A,b) &= \text{span}\{ b,Ab,\dots,A^{r-1}b,A^rb,A^{r+1}b,\dots,A^{s-1}b \} \; , 
\end{align*}
we can easily see that the $r$ vectors in the span of $\mathcal{K}_r(A,b)$ are equal to the first $r$ vectors in the span of $\mathcal{K}_s(A,b)$, so, $K_r(A,b) = K_s(A,b)[1:r]$. In addition, since $Q_s$ is the orthogonal matrix resulting from the QR decomposition of $K_s(A,b)$, by definition we have that the first $r$ columns of $Q_s$ form an orthonormal basis for the span of the first $r$ columns of $K_s(A,b)$, and so we have the following result:
\begin{align}\label{eq:B2_first_result}
    K_r(A,b) = K_s(A,b)[1:r] = Q_s[1:r] R_s[1:r,1:r] \; .
\end{align}

The result above is true for all $1 \leq r \leq s$, so if we set $r$ such that $1 \leq r \leq s-1$, we will also have:
\begin{align}\label{eq:B2_second_result}
    K_{r+1}(A,b) = Q_s[1:r'] R_s[1:r',1:r'] \; ,
\end{align}
where $r' = r+1$. \\

For any $r \in \mathbb{N}_{>0}$, we know that $K_{r+1}(A,b) = [b,A K_r(A,b)]$. With this and thanks to the previous two results, we can write the following:
\begin{align*}
    K_{r+1}(A,b) &= [b,A K_r(A,b)] \\
    &= [b,A Q_s[1:r] R_s[1:r,1:r]] \\
    &= Q_s[1:r'] R_s[1:r',1:r'] \; ,
\end{align*}
where we go from the first to the second line by using equation \ref{eq:B2_first_result} and we go from the second to the third line thanks to equation \ref{eq:B2_second_result}. If we rewrite the equality between the second and third lines from the expression above in the following more compact way $F = QR$, we can write that $F_0 = Q R_0$, where the $0$ in index means that we have removed the first column from the corresponding matrix. This result is true because from the equation $F = QR$, we know that the $k^{th}$ column of $F$ is equal to $Q$ times the $k^{th}$ column of $R$. From this, and by reusing the original equality between the second and third lines from the expression above, we finally find that for each $1 \leq r \leq s-1$,
\begin{align*}
    [b,A Q_s[1:r] R_s[1:r,1:r]] &= Q_s[1:r'] R_s[1:r',1:r'] \\
    &\Longleftrightarrow \\
    A Q_s[1:r] R_s[1:r,1:r] &= Q_s[1:r'] R_s[1:r',2:r'] \; ,
\end{align*}
where $r' = r+1$.
\begin{flushright}
    $\qed$
\end{flushright}



\subsection*{B3}

Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, $s = \text{dim}(\mathcal{K}_n(A,b))$ and $(Q_s,R_s)$ be a QR decomposition of $K_s(A,b)$. From \textbf{B2}, we know that for each $1 \leq r \leq s-1$, we have:
\begin{align*}
    A Q_s[1:r] R_s[1:r,1:r] = Q_s[1:r'] R_s[1:r',2:r'] \; ,
\end{align*}
where $r' = r+1$. \\

The matrix $R_s[1:r,1:r]$ is invertible since it is an upper triangular matrix of size $r$ times $r$, that has nonzero elements on its diagonal, as proved in section \textbf{B1}. Because of this, we can rewrite the expression above as:
\begin{align*}
    A Q_s[1:r] &= Q_s[1:r'] R_s[1:r',2:r'] \displaystyle\left(R_s[1:r,1:r]\right)^{-1} \\
    &= Q_s[1:r'] H_r \; ,
\end{align*}
where $H_r = R_s[1:r',2:r'] \displaystyle\left(R_s[1:r,1:r]\right)^{-1} \in \mathbb{R}^{r' \times r}$. Let's now show that $H_r$ is a Hessenberg matrix. \\

Since $R_s[1:r',1:r']$ is an upper triangular matrix, the matrix $R_s[1:r',2:r']$, obtained by removing from $R_s[1:r',1:r']$ it's first column, must be a Hessenberg matrix since we will now have $R_s[1:r',2:r'][i,j] = 0$ whenever $i > j + 1$. In addition, since $R_s[1:r,1:r]$ is an upper triangular matrix, it's inverse $(R_s[1:r,1:r])^{-1}$ is also an upper triangular matrix. Finally, since $H_r$ is the product of $R_s[1:r',2:r']$ and of $(R_s[1:r,1:r])^{-1}$, $H_r$ must also be a Hessenberg matrix. Indeed, the entry in row $i$, column $j$ of $H_r$ is given by:
\begin{align*}
    H_r[i,j] = \sum_k R_s[1:r',2:r'][i,k] \, (R_s[1:r,1:r])^{-1}[k,j] \; .
\end{align*} 
Since $R_s[1:r',2:r']$ is a Hessenberg matrix, $R_s[1:r',2:r'][i,k] = 0$ for $i > k+1$. Additionally, since $(R_s[1:r,1:r])^{-1}$ is an upper triangular matrix, $(R_s[1:r,1:r])^{-1}[k,j] = 0$ for $k > j$. So, for $i > j+1$, the non-zero elements of $R_s[1:r',2:r']$ and $(R_s[1:r,1:r])^{-1}$ do not overlap and so $H_r[i,j] = 0$ for $i > j+1$. This thus proves that $H_r$ is indeed a Hessenberg matrix, and so for each $1 \leq r \leq s-1$, there is a Hessenberg matrix $H_r \in \mathbb{R}^{r' \times r}$ such that $A Q_s[1:r] = Q_s[1:r'] H_r$.
\begin{flushright}
    $\qed$
\end{flushright}


\subsection*{B4}
Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, and let $(q_1,...,q_s)$ be the output of Arnoldi's method with input $(A,b)$.\\
We can show that for each $1\leq r \leq s$, span$\{q_1,...,q_r\} \subseteq \mathcal{K}_r(A,b)$. In other words, we want to show that the span of the orthonormal sequence of vectors is a subset of the Krylov subspace of order $r$ generated by $A$ and $b$.\\

To show that one subspace is a subset of another subspace, we need to demonstrate that every vector in the first subspace also belongs to the second one. Here we will take advantage of the fact that the subspace is constructed through an iterative process and prove it by induction.\\

First, as base case, let's take $r=1$.\\
In this case, we have 
$ \mathcal{K}_1(A,b) = \text{span}\{ b\} $
and, according to Arnoldi's method, 
$\text{span}\{q_1\} = \text{span}\left\{\frac{b}{\|b\|}\right\}$.\\
We know that the span of a set of vectors is the collection of all possible linear combinations of those vectors. In our specific case, \( \text{span}\{b\} \) contains all scalar multiples of \( b \), i.e., $\text{span}\{b\} = \{ \alpha b \mid \alpha \in \mathbb{R} \}$. Similarly, we have
  \[
  \text{span}\left\{\frac{b}{\|b\|}\right\} = \{ \alpha \frac{b}{\|b\|} \mid \alpha \in \mathbb{R} \}.
  \]
Since \( \frac{b}{\|b\|} \) is simply a scalar multiple of \( b \), we can express any vector in \( \text{span}\left\{\frac{b}{\|b\|}\right\} \) as a scalar multiple of \( b \). Specifically:
\[
\alpha \frac{b}{\|b\|} = \beta b \quad \text{where} \quad \beta = \frac{\alpha}{\|b\|}.
\]
Therefore, every element in \( \text{span}\left\{\frac{b}{\|b\|}\right\} \) is also in \( \text{span}\{b\} \), and we indeed have that 
$
\text{span}\left\{\frac{b}{\|b\|}\right\} \subseteq \text{span}\{b\}.
$\\

Next, as inductive hypothesis, let's assume that span$\{q_1,...,q_r\} \subseteq \mathcal{K}_r(A,b)$ holds true for each $1\leq r < s$.\\
We now would like to show that it is also true for $r+1$, i.e., span$\{q_1,...,q_{r+1}\} \subseteq \mathcal{K}_{r+1}(A,b)$.\\

For that, let's look at the start of the \( (r+1) \)-th iteration of Arnoldi's method: the vector\( v \) is computed as $v = Aq_r$.
  Since we assumed \( \text{span}\{q_1, \dots, q_r\} \subseteq K_r(A, b) \) true, we know that
  \[
  q_r \in K_r(A, b) = \text{span}\{b, Ab, \dots, A^{r-1}b\}
  \]
  and by applying \( A \) to \( q_r \), like it is done in the algorithm, we get $Aq_r \in \text{span}\{Ab, A^2b, \dots, A^r b\} = K_{r+1}(A, b)$.\\
  
Then, the algorithm orthogonalizes \( v \) against the previous vectors \( q_1, \dots, q_r \) and the resulting vector \( w \) is still in \( K_{r+1}(A, b) \). We then have that, if \( w \neq 0 \), the vector \( w \) is normalized in order to get \( q_{r+1} \):
  \[
  q_{r+1} = \frac{w}{\|w\|}
  \]
  From which it naturally follows that \( q_{r+1} \in K_{r+1}(A, b) \) as well, proving that \( \text{span}\{q_1, \dots, q_{r+1}\} \subseteq K_{r+1}(A, b) \) and completing the induction step.\\

  All of this proves by induction that we indeed have span$\{q_1,...,q_r\} \subseteq \mathcal{K}_r(A,b)$, for each $1\leq r \leq s$,.

  \begin{flushright}
    $\qed$
\end{flushright}

\subsection*{B5}

Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, and let $(q_1,...,q_s)$ be the output of Arnoldi's method with input $(A,b)$.\\
We now want to show that for each \(1 \leq r \leq s\), \( \text{span}\{q_1, \dots, q_r\} = K_r(A, b)\), i.e., the span of orthonormal vectors produced by the Arnoldi's algorithm is not just a subset, but equal to the Krylov subspace.\\

For that, let's recall the results found previously.\\
Specifically, from \textbf{B4}, we proved that \( \text{span}\{q_1, \dots, q_r\} \subseteq K_r(A, b) \) for each \(1 \leq r \leq s\), based on the fact that each vector \(q_r\) of the Arnoldi's method is generated as the orthonormalization of \( A q_{r-1} \), ensuring that it lies in \(K_r(A, b)\). Then, in \textbf{A2}, we saw that \( \dim(K_r(A, b)) = r \) for each \(1 \leq r \leq s\), meaning that the Krylov subspace \(K_r(A, b)\) has exactly \(r\) linearly independent vectors.\\
   
Going back to our problem, we already know that the sequence \( (q_1, q_2, \dots, q_r) \) generated by Arnoldi's method is orthonormal. Therefore, the vectors \( q_1, \dots, q_r \) are linearly independent, and the dimension of their span is exactly \(r\). This leads to the following equality:
   
$$ \dim(\text{span}\{q_1, \dots, q_r\}) = r = \dim(K_r(A, b)) $$
However, as it has been said in \textbf{B4}, \( \text{span}\{q_1, \dots, q_r\} \subseteq K_r(A, b) \), and therefore the only way for this to hold true is if $\text{span}\{q_1, \dots, q_r\} = K_r(A, b)$. This follows from the fact that if two subspaces have the same dimension and if one of them is a subset of the other, then they must be equal.\\

     All of this proves indeed that \( \text{span}\{q_1, \dots, q_r\} = K_r(A, b)\), for each \(1 \leq r \leq s\).

\begin{flushright}
    $\qed$
\end{flushright}

\subsection*{B6}
Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, and let $(q_1,...,q_s)$ be the output of Arnoldi's method with input $(A,b)$.\\
We now want to show that $s = \text{dim}(\mathcal{K}_n(A,b))$, i.e., the number of vectors \( q_1, \dots, q_s \) in the sequence generated by the Arnoldi method is equal to \( \dim(K_n(A, b)) \).\\

For that, recall that the Arnoldi algorithm iteratively constructs an orthonormal sequence of vectors $(q_1,...,q_s)$ such that each vector \( q_r \) lies in the Krylov subspace \( K_r(A, b) \), for $1\leq r \leq s$. The algorithm stops whenever \( w = v - v' = 0 \), with \( v = A q_s \) and \( v' \) the orthogonal projection of \( v \) on span\( \{q_1, \dots, q_s\} \), which only happens once the computed vector \( v \) is already in the span of \( \{q_1, \dots, q_s\} \), meaning that no new orthogonal vector can be generated. We also know, from \textbf{A2}, that \( \dim(K_n(A, b)) = s \) for some \( s \), where \( s \) is the maximum number of linearly independent vectors that can be formed from \( \{b, Ab, \dots, A^{n-1}b\} \).\\

Therefore, to prove that the number of vectors \( q_1, \dots, q_s \) generated by the Arnoldi's method is equal to \( \dim(K_n(A, b)) \), we can simply show that the Arnoldi's method terminates at \( s=\dim(K_n(A, b)) \).\\
Indeed, we said based on \textbf{A2} that the dimension of the full Krylov subspace \( K_n(A, b) \) is \( s \). This means that after generating \( s \) orthonormal vectors, applying \( A \) to \( q_s \) will result in a vector that is a linear combination of the previous vectors \( \{q_1, \dots, q_s\} \). Thus, the algorithm stops, as no further linearly independent vector can be produced and $w = 0$, which gives us a total number $s$ of orthonormal vectors generated. \\
All of this concludes that \( s = \dim(K_n(A, b)) \), as required.

\begin{flushright}
    $\qed$
\end{flushright}


\subsection*{B7}
Let $A \in \mathbb{R}^{n \times n}$, $b \in \mathbb{R}^n \setminus \{0\}$, and let $(q_1,...,q_s)$ be the output of Arnoldi's method with input $(A,b)$.\\
We now want to show that $Q_s \overset{\Delta}{=} [q_1,...,q_s]$ provides a QR decomposition of $K_s(A,b)$, i.e., $[b, Ab, \dots, A^{s-1}b] = Q_sR_s$ for some upper-triangular matrix $R_s$.\\

To prove this, recall from \textbf{B5} that for each \( 1 \leq r \leq s \), \( \text{span}\{q_1, \dots, q_r\} = \mathcal{K}_r(A, b) = \text{span}\{b, Ab, \dots, A^{r-1}b\} \). This result tells us that the vectors \( q_1, \dots, q_s \) span the Krylov subspace \( \mathcal{K}_s(A, b) \), i.e., the columns of \( Q_s \) span the same subspace as \( K_s(A, b) \). We also know that he output vectors \( q_1, \dots, q_s \) from Arnoldi’s method are orthonormal, meaning that the matrix \( Q_s = [q_1, ..., q_s] \) satisfies \( Q_s^\top Q_s = I_s \), where \( I_s \) is the identity matrix of size \( s \times s \).\\

By definition, a QR decomposition $A = QR$ of a square matrix $A$, must be such that the matrix \( Q \) contains an orthonormal basis for the subspace spanned by the columns of \( A\) and the matrix $R$ contains the coefficients expressing the columns of $A$ as linear combinations of the columns of $Q$. We showed that it is indeed the case here from the results of \textbf{B5} since the columns of $Q_s$ span the same subspace as \( K_s(A, b) \), i.e. $Q_s$ contains an orthonormal basis for the subspace spanned by the columns of $K_s(A,b)$. From this last statement, we know that each column of \( K_s(A, b) \) can therefore be expressed as a linear combination of the orthonormal vectors \( q_1, ..., q_s \). It results that this linear combination must indeed be captured in the matrix \( R_s \), which holds the coefficients for expressing the columns of \( K_s(A, b) \) in terms of the columns of \( Q_s \).\\

Now, let's prove that the matrix $R_s$ is indeed an upper-triangular matrix.\\
   The Arnoldi process iteratively constructs each new vector \( q_{r+1} \) as an orthogonal projection of \(v = A q_r \) onto the span of \( \{q_1, ..., q_r\} \), and this projection can be expressed as a linear combination of the previous vectors as follows:
   $$ v' = \sum_{i=1}^r \langle {v,q_i} \rangle q_i$$
   If we decide to construct the matrix \( R_s \) with the coefficients of the linear combinations of each step, and since we can clearly see from the above expression that a new column in \( K_s(A, b) \) (i.e., \( A^{r-1}b \)) is projected onto the already constructed orthonormal vectors \( \{q_1, ..., q_r\} \), it therefore means that no information is added below the diagonal as no inner products can be performed with the (not constructed yet) vectors $q_{r+1},...,q_s$. We then have shown that $R_s$ is upper-triangular as desired.\\
   
This completes the proof that \( Q_s = [q_1, ..., q_s] \) provides a QR decomposition of \( K_s(A, b) \).

\begin{flushright}
    $\qed$
\end{flushright}


\subsection*{D1}

It comes

\begin{align*}
|| Ax - \lambda x || &= || A Q y - \lambda Q y || \\
&= || Q H y + \beta q e_r^T y -Q \lambda y || \\
&= || Q H y + \beta q e_r^T y -Q H y || \\
&= || \beta q e_r^T y || \\
&= | \beta e_r^T y | \cdot ||q|| \\
&= | \beta er_r^T y |
\end{align*}

Where the last equality holds because $[ Q,q]^T [Q, q] = I \implies q^T q = ||q||^2 = 1$. \qed


\end{document}








